{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('post_debate_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df1[df1['predictor']=='pos'].reset_index()\n",
    "df_neg = df1[df1['predictor']=='neg'].reset_index()\n",
    "df_gen = df1[df1['predictor']=='gen'].reset_index()\n",
    "\n",
    "pos_string = df_pos['text']\n",
    "neg_string = df_neg['text']\n",
    "gen_string = df_gen['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "for p_tweet in pos_string:\n",
    "    (tweet_tokenizer.tokenize(p_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "## Importing stopwords \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "## Importing porter stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "## Importing the tokenizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    ## Cleaing up RTs\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    ## Removing hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    ## Removing hastags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            ##stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(word)\n",
    " \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary\n",
    " \n",
    "# positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_string:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    \n",
    "    \n",
    "## negative tweets\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_string:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    "    \n",
    "## general tweets\n",
    "\n",
    "gen_tweets_set = []\n",
    "for tweet in gen_string:\n",
    "    gen_tweets_set.append((bag_of_words(tweet), 'gen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14267 120\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle \n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    " \n",
    "test_set = pos_tweets_set[:40] + neg_tweets_set[:40] + gen_tweets_set[:40]\n",
    "train_set = pos_tweets_set[10:] + neg_tweets_set[10:] + gen_tweets_set[10:]\n",
    "print(len(train_set),len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8583333333333333\n",
      "Most Informative Features\n",
      "                disaster = True              neg : pos    =    212.4 : 1.0\n",
      "                complete = True              neg : pos    =    212.1 : 1.0\n",
      "                    grow = True              neg : pos    =    209.9 : 1.0\n",
      "                  google = True              neg : pos    =    149.4 : 1.0\n",
      "                   backs = True              pos : neg    =    142.3 : 1.0\n",
      "                 notmeus = True              pos : gen    =    134.7 : 1.0\n",
      "                      en = True              gen : neg    =    130.3 : 1.0\n",
      "                      de = True              gen : neg    =    119.8 : 1.0\n",
      "        berniebeatstrump = True              pos : gen    =    119.3 : 1.0\n",
      "                     que = True              gen : neg    =    113.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) # Output: 0.765\n",
    " \n",
    "print (classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Result: pos\n",
      "Negative Tweet: 0.00025418195725589746\n",
      "Positive Tweet: 0.9996675036706884\n",
      "General Tweet: 7.831437205665417e-05\n"
     ]
    }
   ],
   "source": [
    "test_tweet1 = \"Bernie Sanders Just Got The Latino Endorsement\"\n",
    "custom_tweet_set = bag_of_words(test_tweet1)\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print ('Overall Result: ' + str(prob_result.max())) \n",
    "print ('Negative Tweet: ' + str(prob_result.prob(\"neg\"))) \n",
    "print ('Positive Tweet: ' + str(prob_result.prob(\"pos\")))\n",
    "print ('General Tweet: ' + str(prob_result.prob(\"gen\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Result: pos\n",
      "Negative Tweet: 0.05982587862355717\n",
      "Positive Tweet: 0.9400752929779851\n",
      "General Tweet: 9.88283984568889e-05\n"
     ]
    }
   ],
   "source": [
    "test_tweet2 = \"Amy Klobuchar\"\n",
    "custom_tweet_set = bag_of_words(test_tweet2)\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print ('Overall Result: ' + str(prob_result.max())) \n",
    "print ('Negative Tweet: ' + str(prob_result.prob(\"neg\"))) \n",
    "print ('Positive Tweet: ' + str(prob_result.prob(\"pos\")))\n",
    "print ('General Tweet: ' + str(prob_result.prob(\"gen\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Result: pos\n",
      "Negative Tweet: 0.4729993399485273\n",
      "Positive Tweet: 0.5172007261627484\n",
      "General Tweet: 0.009799933888723748\n"
     ]
    }
   ],
   "source": [
    "test_tweet3 = \"Bernie sucks!\"\n",
    "custom_tweet_set = bag_of_words(test_tweet3)\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print ('Overall Result: ' + str(prob_result.max())) \n",
    "print ('Negative Tweet: ' + str(prob_result.prob(\"neg\"))) \n",
    "print ('Positive Tweet: ' + str(prob_result.prob(\"pos\")))\n",
    "print ('General Tweet: ' + str(prob_result.prob(\"gen\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_full_name = ['Joe Biden', 'Amy Klobuchar', 'Pete Buttigieg', 'Bloomberg', 'Bernie Sanders', 'Elizabeth Warren']\n",
    "candidates_last_name = ['Biden', 'Klobuchar', 'Buttigieg', 'Bloomberg', 'Sanders', 'Warren']\n",
    "\n",
    "def get_sentiment(name):\n",
    "    \n",
    "    final_results = []\n",
    "    \n",
    "    for potential_pres in name:\n",
    "        custom_tweet_set = bag_of_words(potential_pres)\n",
    "        prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "        name = potential_pres\n",
    "        results = str(prob_result.max()) \n",
    "        neg_results = str(prob_result.prob(\"neg\"))\n",
    "        pos_results = str(prob_result.prob(\"pos\"))\n",
    "        create_dict = {'Name': name, 'Results': results, 'Negative Probability': neg_results, 'Positive Probability': pos_results}\n",
    "        final_results.append(create_dict)\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sentiment = get_sentiment(candidates_last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.DataFrame(post_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df['scenario'] = 'post_debate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.to_csv('post_debate_candidate_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = df1[df1['predictor']=='pos']\n",
    "negatives = df1[df1['predictor']=='neg']\n",
    "general = df1[df1['predictor']=='gen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of predicted general tweets were: 2488 \n",
      "The amount of predicted positive tweets were: 5363 \n",
      "The amount of predicted negative tweets were: 6446\n"
     ]
    }
   ],
   "source": [
    "print(\"The amount of predicted general tweets were: {} \\nThe amount of predicted positive tweets were: {} \\nThe amount of predicted negative tweets were: {}\".format(len(general),len(positives),len(negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'post_debate_model.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 potenti = True              neg : pos    =    268.1 : 1.0\n",
      "                 complet = True              neg : pos    =    202.6 : 1.0\n",
      "                      en = True              gen : neg    =    159.8 : 1.0\n",
      "                  notmeu = True              pos : gen    =    140.0 : 1.0\n",
      "                    fear = True              neg : pos    =    127.9 : 1.0\n",
      "        berniebeatstrump = True              pos : gen    =    127.4 : 1.0\n",
      "                     del = True              gen : neg    =    124.7 : 1.0\n",
      "           berniewoniowa = True              pos : neg    =    123.2 : 1.0\n",
      "                      de = True              gen : neg    =    120.5 : 1.0\n",
      "                    bigu = True              pos : neg    =    118.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_full_name = ['Joe Biden', 'Amy Klobuchar', 'Pete Buttigieg', 'Bloomberg', 'Bernie Sanders', 'Elizabeth Warren']\n",
    "candidates_last_name = ['Biden', 'Klobuchar', 'Buttigieg', 'Bloomberg', 'Sanders', 'Warren']\n",
    "\n",
    "def get_sentiment_pickle(name):\n",
    "    \n",
    "    final_results = []\n",
    "    \n",
    "    for potential_pres in name:\n",
    "        custom_tweet_set = bag_of_words(potential_pres)\n",
    "        prob_result = loaded_model.prob_classify(custom_tweet_set)\n",
    "        name = potential_pres\n",
    "        results = str(prob_result.max()) \n",
    "        neg_results = str(prob_result.prob(\"neg\"))\n",
    "        pos_results = str(prob_result.prob(\"pos\"))\n",
    "        create_dict = {'Name': name, 'Results': results, 'Negative Probability': neg_results, 'Positive Probability': pos_results}\n",
    "        final_results.append(create_dict)\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Name': 'Biden', 'Results': 'pos', 'Negative Probability': '0.41439243029682404', 'Positive Probability': '0.5696958819679321'}, {'Name': 'Klobuchar', 'Results': 'pos', 'Negative Probability': '0.1793254022356111', 'Positive Probability': '0.8061365276892786'}, {'Name': 'Buttigieg', 'Results': 'pos', 'Negative Probability': '0.3715291786987502', 'Positive Probability': '0.5941046002990142'}, {'Name': 'Bloomberg', 'Results': 'neg', 'Negative Probability': '0.5326488253926238', 'Positive Probability': '0.4096469210687091'}, {'Name': 'Sanders', 'Results': 'pos', 'Negative Probability': '0.2084615288329245', 'Positive Probability': '0.7547557660546834'}, {'Name': 'Warren', 'Results': 'pos', 'Negative Probability': '0.23843938422971764', 'Positive Probability': '0.7319077032466412'}]\n"
     ]
    }
   ],
   "source": [
    "get_sentiment_pickle = get_sentiment(candidates_last_name)\n",
    "\n",
    "print(get_sentiment_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.DataFrame(get_sentiment_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Negative Probability</th>\n",
       "      <th>Positive Probability</th>\n",
       "      <th>Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biden</td>\n",
       "      <td>0.41439243029682404</td>\n",
       "      <td>0.5696958819679321</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Klobuchar</td>\n",
       "      <td>0.1793254022356111</td>\n",
       "      <td>0.8061365276892786</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buttigieg</td>\n",
       "      <td>0.3715291786987502</td>\n",
       "      <td>0.5941046002990142</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>0.5326488253926238</td>\n",
       "      <td>0.4096469210687091</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sanders</td>\n",
       "      <td>0.2084615288329245</td>\n",
       "      <td>0.7547557660546834</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Warren</td>\n",
       "      <td>0.23843938422971764</td>\n",
       "      <td>0.7319077032466412</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name Negative Probability Positive Probability Results\n",
       "0      Biden  0.41439243029682404   0.5696958819679321     pos\n",
       "1  Klobuchar   0.1793254022356111   0.8061365276892786     pos\n",
       "2  Buttigieg   0.3715291786987502   0.5941046002990142     pos\n",
       "3  Bloomberg   0.5326488253926238   0.4096469210687091     neg\n",
       "4    Sanders   0.2084615288329245   0.7547557660546834     pos\n",
       "5     Warren  0.23843938422971764   0.7319077032466412     pos"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(tweet):\n",
    "    words = clean_tweets(tweet)    \n",
    "    return words \n",
    "\n",
    "candidate_names = ['biden', 'klobuchar', 'buttigieg', 'bloomberg', 'sanders', 'warren',\n",
    "                  'joe biden', 'amy klobuchar', 'pete buttigieg', 'michael bloomberg',\n",
    "                   'bernie sanders', 'elizabeth warren',\n",
    "                  'joe','amy','pete','michael','bernie','liz','elizabeth','bernard']\n",
    "list_comp = []\n",
    "found_match = []\n",
    "\n",
    "for i,v in df1.iterrows():\n",
    "    text = v['text']\n",
    "    tokenize = get_tokens(text)\n",
    "    list_comp = [name for name in tokenize if name in candidate_names]\n",
    "    found_match.append(list_comp)\n",
    "    \n",
    "df1['associated_candidate'] = found_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('post_debate_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
